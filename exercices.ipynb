{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "592121b2",
   "metadata": {},
   "source": [
    "### EXERCICES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5624da85",
   "metadata": {},
   "source": [
    "génération à partir de chatgpt...\n",
    "\n",
    "\n",
    "Exercice1 : un notebook python où vous arrivez à générer/visualiser une trajectoire.\n",
    "\n",
    "Exercice2 : définir / implémenter des métriques statistiques et des tests d'identification des browniens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d366256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89257122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour simuler normale, on a box muller, marsaglia, ziggurat, etc. \n",
    "# mais on peut utiliser la fonction de numpy standatd_normal qui est optimisée et rapide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d61a2bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_brownian(mu=0.0, sigma=1.0, T=1.0, n=252, x0=0.0, seed=None):\n",
    "    \"\"\"\n",
    "    Simule X_t = x0 + mu t + sigma W_t sur [0, T] avec n pas.\n",
    "    Retourne t(n+1,), X(n+1,), dt.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    dt = T / n\n",
    "    dX = mu * dt + sigma * np.sqrt(dt) * rng.standard_normal(n)\n",
    "    X = np.empty(n + 1)\n",
    "    X[0] = x0\n",
    "    X[1:] = x0 + np.cumsum(dX)\n",
    "    t = np.linspace(0.0, T, n + 1)\n",
    "    return t, X, dt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7658c1e",
   "metadata": {},
   "source": [
    "## on doit vérifier les 4 propositions de la définition donnée dans le polycopié sur les méthodes monte carlo:\n",
    "\n",
    "#### Un processus stochastique W = (Wt)t∈R+ est un mouvement brownien standard si, et seulement si,\n",
    "\n",
    "### (1) W est issu de 0, i.e., W0 = 0 p.s.,\n",
    "\n",
    "### (2) W est à trajectoires continues,\n",
    "\n",
    "### (3) W est à accroissements indépendants, i.e., pour toutes séquences 0 = t0 < t1 < ... < tn, les variables # Wt2−Wt1, ..., #Wtn−Wtn−1 # sont indépendantes,\n",
    "\n",
    "### (4) W est à accroissements stationnaires,i.e., pour tous s < t la valeur de Wt −Ws ne dépend que de la valeur de t − s et Wt −Ws ∼ N (0,t − s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b920fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pour le premier point, on peut faire une vérification brute que W0 = 0 p.s. \n",
    "# en regardant les valeurs de W0 dans la simulation et en calculant leur maximum absolu. \n",
    "# On devrait trouver que ce maximum est très proche de zéro, ce qui confirme \n",
    "# que W0 est effectivement égal à zéro presque sûrement dans la simulation.\n",
    "\n",
    "def test_W0_is_zero(W):\n",
    "    print(\"\\n=== (1) Test W0 = 0 p.s. ===\")\n",
    "    W0 = W[:, 0]\n",
    "    max_abs = np.max(np.abs(W0))\n",
    "    print(f\"[A] max |W0| = {max_abs:.3e} (devrait être 0 en simulation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69f4f984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on pourrait aussi faire le test de lilliefors, de shapiro wilk et de anderson darling pour la normalité des incréments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2634080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_bm_paths(m=2000, n=2000, T=1.0, seed=123):\n",
    "    \"\"\"\n",
    "    Simule m trajectoires d'un Brownien standard sur [0,T] avec n pas.\n",
    "    Retourne:\n",
    "      t: (n+1,)\n",
    "      W: (m, n+1)\n",
    "      dW: (m, n)  (accroissements)\n",
    "      dt\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    dt = T / n\n",
    "    dW = np.sqrt(dt) * rng.standard_normal(size=(m, n))\n",
    "    W = np.zeros((m, n + 1), dtype=float)\n",
    "    W[:, 1:] = np.cumsum(dW, axis=1)\n",
    "    t = np.linspace(0.0, T, n + 1)\n",
    "    return t, W, dW, dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "723fa272",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pour le second point, on peut faire plusieurs diagnostics de continuité approchés, par exemple:\n",
    "## - regarder la distribution des incréments dW et vérifier qu'ils sont bien petits et tendent vers 0 quand dt -> 0\n",
    "## - calculer le \"jump ratio\" = max(dW^2) / sum(dW^2) pour chaque trajectoire, qui doit tendre vers 0 quand n augmente\n",
    "## - faire un contrôle (grossier) de \"pas de saut\" via un seuil sur les incréments, par exemple en vérifiant que \n",
    "# la fraction d'incréments |dW| > 6*sqrt(dt) est très faible, ce qui serait anormal pour un processus continu\n",
    "\n",
    "def continuity_diagnostics(m=2000, n=2000, T=1.0, seed=123):\n",
    "    print(\"\\n=== (2) Diagnostics de continuité (approchés) ===\")\n",
    "\n",
    "    # Méthode A: taille des incréments vs raffinement du pas\n",
    "    _, _, dW1, dt1 = simulate_bm_paths(m=m, n=n, T=T, seed=seed)\n",
    "    _, _, dW2, dt2 = simulate_bm_paths(m=m, n=4 * n, T=T, seed=seed + 1)\n",
    "\n",
    "    max_inc1 = np.max(np.abs(dW1), axis=1)  # par trajectoire\n",
    "    max_inc2 = np.max(np.abs(dW2), axis=1)\n",
    "\n",
    "    q = [0.5, 0.9, 0.99]\n",
    "    print(\"[A] Quantiles de max|dW| par trajectoire (doit diminuer quand dt diminue):\")\n",
    "    for qq in q:\n",
    "        print(f\"    q={qq:.2f}: dt={dt1:.2e} -> {np.quantile(max_inc1, qq):.4f} | \"\n",
    "              f\"dt={dt2:.2e} -> {np.quantile(max_inc2, qq):.4f}\")\n",
    "\n",
    "    # Méthode B: \"jump ratio\" = max(dW^2) / sum(dW^2)\n",
    "    # Pour un processus continu, ce ratio tend vers 0 quand le maillage se raffine.\n",
    "    ratio1 = np.max(dW1**2, axis=1) / np.sum(dW1**2, axis=1)\n",
    "    ratio2 = np.max(dW2**2, axis=1) / np.sum(dW2**2, axis=1)\n",
    "    print(\"[B] Quantiles de max(dW^2)/sum(dW^2) (tend vers 0 quand n augmente):\")\n",
    "    for qq in q:\n",
    "        print(f\"    q={qq:.2f}: n={n} -> {np.quantile(ratio1, qq):.4e} | \"\n",
    "              f\"n={4*n} -> {np.quantile(ratio2, qq):.4e}\")\n",
    "\n",
    "    # Méthode C: contrôle (grossier) de \"pas de saut\" via seuil\n",
    "    # si un processus avait des sauts, on verrait des incréments anormalement grands.\n",
    "    thr = 6.0 * np.sqrt(dt1)  # seuil ~ 6 sigma au pas dt1, ce qui est assez fin déjà!!\n",
    "    frac_big = np.mean(np.abs(dW1) > thr)\n",
    "    print(f\"[C] Fraction d'incréments |dW| > 6*sqrt(dt) : {frac_big:.3e} (devrait être très faible)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac5906a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pour le troisième point, pour vérifier l'indépendance des accroissements, on peut faire plusieurs tests:\n",
    "## - calculer la corrélation lag-1 (Pearson) sur tout le panel d'incréments, qui doit être proche de 0\n",
    "## - faire un test de Ljung-Box sur une trajectoire pour vérifier l'absence d'autocorrélation\n",
    "## - faire un test par permutation sur la corrélation pour vérifier que la corrélation observée est significative ou pas\n",
    "## - vérifier l'indépendance entre blocs disjoints d'incréments, \n",
    "# par exemple en calculant la corrélation entre la somme des incréments sur [0,T/2] et la somme sur [T/2,T], \n",
    "# qui doit être proche de 0 pour un processus à accroissements indépendants.\n",
    "\n",
    "def independence_tests(dW, max_lag=20, seed=0):\n",
    "    print(\"\\n=== (3) Tests d'indépendance des accroissements ===\")\n",
    "\n",
    "    # A) Corrélation lag-1 (Pearson) sur tout le panel\n",
    "    x = dW[:, :-1].ravel()\n",
    "    y = dW[:, 1:].ravel()\n",
    "    corr = np.corrcoef(x, y)[0, 1]\n",
    "    print(f\"[A] Corr(dW_t, dW_{'{'}t+dt{'}'}) ≈ {corr:.4f} (devrait être ~0)\")\n",
    "\n",
    "    # B) Ljung-Box sur une trajectoire (test d'absence d'autocorr)\n",
    "    from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "    if acorr_ljungbox is not None:\n",
    "        series = dW[0]  # 1 trajectoire\n",
    "        L = min(max_lag, len(series) // 5 if len(series) >= 5 else 1)\n",
    "        lb = acorr_ljungbox(series, lags=L, return_df=True)\n",
    "        p_last = float(lb[\"lb_pvalue\"].iloc[-1])\n",
    "        print(f\"[B] Ljung-Box (traj 0) jusqu'au lag {L}: p-value={p_last:.3g} (grand => pas d'autocorr détectée)\")\n",
    "    else:\n",
    "        print(\"[B] statsmodels non dispo -> Ljung-Box ignoré\")\n",
    "\n",
    "    # C) Test par permutation sur corrélation (robuste, ne suppose pas la normalité)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = rng.permutation(len(y))\n",
    "    corr_perm = np.corrcoef(x, y[idx])[0, 1]\n",
    "    print(f\"[C] Corr après permutation (doit ressembler à 0): {corr_perm:.4f}\")\n",
    "\n",
    "    # D) Indépendance entre blocs disjoints: somme des incréments sur [0,T/2] vs [T/2,T]\n",
    "    mid = dW.shape[1] // 2\n",
    "    A = dW[:, :mid].sum(axis=1)\n",
    "    B = dW[:, mid:].sum(axis=1)\n",
    "    corr_blocks = np.corrcoef(A, B)[0, 1]\n",
    "    print(f\"[D] Corr( sum dW bloc1 , sum dW bloc2 ) ≈ {corr_blocks:.4f} (devrait être ~0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48f82a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pour le quatrième point, pour vérifier la normalité des accroissements, on peut faire plusieurs tests:\n",
    "## - Kolmogorov-Smirnov vs N(0,1) sur les incréments standardisés Z = (W_{t+k}-W_t)/sqrt(k*dt) pour plusieurs k\n",
    "## - faire un histogramme des incréments et le comparer à la densité d'une normale\n",
    "## - faire un Q-Q plot des incréments vs une normale\n",
    "## - faire un test de normalité formel (Shapiro-Wilk, Anderson-Darling, Lilliefors, etc.) sur un échantillon d'incréments\n",
    "\n",
    "def stationary_and_gaussian_tests(W, dt, ks=(1, 2, 5, 10, 25, 50)):\n",
    "    print(\"\\n=== (4) Tests: stationnarité des accroissements + N(0, t-s) ===\")\n",
    "    m, n1 = W.shape[0], W.shape[1] - 1\n",
    "\n",
    "    # Méthode A: pour chaque k, standardiser les incréments: Z = (W_{t+k}-W_t)/sqrt(k*dt) ~ N(0,1)\n",
    "    print(\"[A] Tests sur Z = incréments standardisés (moyenne 0, variance 1, normalité):\")\n",
    "    for k in ks:\n",
    "        if k >= n1:\n",
    "            continue\n",
    "        inc = W[:, k:] - W[:, :-k]               # (m, n1-k+1)\n",
    "        Z = (inc / np.sqrt(k * dt)).ravel()      # pool\n",
    "\n",
    "        meanZ = Z.mean()\n",
    "        varZ = Z.var(ddof=0)\n",
    "\n",
    "        line = f\"    k={k:>3d} (tau={k*dt:.4f}): mean={meanZ:+.3e}, var={varZ:.4f}\"\n",
    "\n",
    "        # Normalité: KS vs N(0,1)\n",
    "        if stats is not None:\n",
    "            ks_stat, ks_p = stats.kstest(Z, \"norm\")  # compare à N(0,1)\n",
    "            line += f\", KS p={ks_p:.3g}\"\n",
    "        print(line)\n",
    "\n",
    "    # Méthode B: stationnarité \"dans le temps\" (même k) :\n",
    "    # comparer les incréments de début vs fin via KS (égalité de distribution)\n",
    "    if stats is not None:\n",
    "        k = min(10, n1 // 4) if n1 >= 40 else 1\n",
    "        inc = (W[:, k:] - W[:, :-k])  # (m, n1-k+1)\n",
    "        L = inc.shape[1]\n",
    "        early = inc[:, : L // 4].ravel()\n",
    "        late  = inc[:, -L // 4 :].ravel()\n",
    "\n",
    "        # On compare distributions (KS) et variances (Levene)\n",
    "        ks_stat, ks_p = stats.ks_2samp(early, late)\n",
    "        lev_stat, lev_p = stats.levene(early, late, center=\"median\")\n",
    "        print(f\"\\n[B] Stationnarité temporelle (k={k}, tau={k*dt:.4f}):\")\n",
    "        print(f\"    KS(early vs late) p={ks_p:.3g} (grand => pas de diff détectée)\")\n",
    "        print(f\"    Levene(variances) p={lev_p:.3g} (grand => variances similaires)\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\n[B] SciPy non dispo -> KS/Levene ignorés\")\n",
    "\n",
    "    # Méthode C: variance-temps Var(W_{t+tau}-W_t) ≈ tau (ici sigma=1)\n",
    "    # Régression log(var) vs log(tau) -> pente ~1\n",
    "    taus = []\n",
    "    vars_tau = []\n",
    "    for k in ks:\n",
    "        if k >= n1:\n",
    "            continue\n",
    "        inc = (W[:, k:] - W[:, :-k]).ravel()\n",
    "        taus.append(k * dt)\n",
    "        vars_tau.append(np.var(inc, ddof=0))\n",
    "\n",
    "    taus = np.asarray(taus, dtype=float)\n",
    "    vars_tau = np.asarray(vars_tau, dtype=float)\n",
    "    slope, intercept = np.polyfit(np.log(taus), np.log(vars_tau), 1)\n",
    "\n",
    "    print(\"\\n[C] Scaling Var(incréments) ~ tau :\")\n",
    "    print(f\"    pente log-log ≈ {slope:.4f} (devrait être ~1 pour Brownien)\")\n",
    "    print(\"    quelques points (tau, var):\")\n",
    "    for tau, vv in zip(taus[:6], vars_tau[:6]):\n",
    "        print(f\"      tau={tau:.4f}, var={vv:.4f}\")\n",
    "    \n",
    "    # Méthode D: Q-Q plot des incréments vs N(0, k*dt) pour un k donné\n",
    "    k = min(10, n1 // 4) if n1 >= 40 else 1\n",
    "    inc = (W[:, k:] - W[:, :-k]).ravel()    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    stats.probplot(inc, dist=\"norm\", plot=plt)\n",
    "    plt.title(f\"Q-Q plot des incréments pour k={k} (tau={k*dt:.4f})\")\n",
    "    plt.show()\n",
    "\n",
    "    # Méthode E: test de normalité formel sur les incréments pour un k donné \n",
    "    from statsmodels.stats.diagnostic import lilliefors\n",
    "    stat, p = lilliefors(inc, dist=\"norm\")  # normalité avec paramètres estimés\n",
    "    print(f\"Test de Lilliefors: stat={stat:.4f}, p={p:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bc0e10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # paramètres\n",
    "    m = 2000   # nb de trajectoires\n",
    "    n = 2000   # pas de temps\n",
    "    T = 1.0\n",
    "    seed = 123\n",
    "\n",
    "    t, W, dW, dt = simulate_bm_paths(m=m, n=n, T=T, seed=seed)\n",
    "\n",
    "    # (1)\n",
    "    test_W0_is_zero(W)\n",
    "\n",
    "    # (2) plusieurs diagnostics (raffinement + ratios)\n",
    "    continuity_diagnostics(m=m, n=n, T=T, seed=seed)\n",
    "\n",
    "    # (3)\n",
    "    independence_tests(dW, max_lag=20, seed=seed)\n",
    "\n",
    "    # (4)\n",
    "    stationary_and_gaussian_tests(W, dt, ks=(1, 2, 5, 10, 25, 50, 100, 200))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mynation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
